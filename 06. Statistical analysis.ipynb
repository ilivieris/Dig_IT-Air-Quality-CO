{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Basic libraries\n",
    "#\n",
    "import time\n",
    "import pandas    as pd\n",
    "import numpy     as np\n",
    "from   tqdm      import tqdm\n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "# Visualization library\n",
    "#\n",
    "import matplotlib.pyplot   as plt \n",
    "\n",
    "\n",
    "# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "#\n",
    "# User libraries\n",
    "#\n",
    "from utils.non_parametric_tests import *\n",
    "from utils.PerformanceProfiles  import *\n",
    "from utils.PerformanceMetrics   import RegressionEvaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "path = 'Predictions/'\n",
    "Files = [f for f in listdir( path ) if isfile(join(path, f))]\n",
    "\n",
    "print('[INFO] Number of methods: ', len(Files) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = []\n",
    "df      = None\n",
    "for file in Files:\n",
    "    # Get method\n",
    "    Method = file.split('.')[0] \n",
    "    print('Model: ', Method)\n",
    "    # Load data\n",
    "    temp = pd.read_csv(path + file, index_col = 0)\n",
    "    \n",
    "    # Evaluation\n",
    "    #\n",
    "    MAE, RMSE, MAPE, SMAPE, R2 = RegressionEvaluation( temp )\n",
    "    Results.append( [Method, MAE, RMSE, MAPE, SMAPE, 100*R2] )\n",
    "    \n",
    "    # Calculate Absolute Errors\n",
    "    temp[ Method ] = (temp.iloc[:,0] - temp.iloc[:,1]).abs().round(4)\n",
    "    # Keep only errors\n",
    "    temp = temp[ Method ]\n",
    "    \n",
    "    if (df is None):\n",
    "        df = temp\n",
    "    else:\n",
    "        df = pd.concat([df, temp], axis=1)\n",
    "        \n",
    "\n",
    "# df.head( 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance evaluation based on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results = pd.DataFrame(data = Results, columns = ['Method', 'MAE', 'RMSE', 'MAPE', 'SMAPE', 'R2'] )\n",
    "\n",
    "Results['MAE']   = Results['MAE'].apply(lambda x: np.round(x,3))\n",
    "Results['RMSE']  = Results['RMSE'].apply(lambda x: np.round(x,3))\n",
    "Results['MAPE']  = Results['MAPE'].apply(lambda x: np.round(x,3))\n",
    "Results['SMAPE'] = Results['SMAPE'].apply(lambda x: np.round(x,3))\n",
    "Results['R2']    = Results['R2'].apply(lambda x: np.round(x,2))\n",
    "\n",
    "\n",
    "Results.sort_values(by = 'SMAPE', ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in df.columns:\n",
    "    df[x] = df[x].apply(lambda x: np.round(x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ ['Convolutional', 'NBeats', 'RF', 'Seq2Seq_LSTM', 'NBeats_diff'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance profiles\n",
    "\n",
    "- Dolan, E. D., & Moré, J. J. (2002). Benchmarking optimization software with performance profiles. Mathematical programming, 91(2), 201-213.\n",
    "ISO 690\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perfprof(df, \n",
    "#          linespecs = ['r-', 'b-', 'g--', 'c-.', 'm-.', 'y--'], \n",
    "         digit       = 1,\n",
    "         legendnames = df.columns,\n",
    "         thmax       = 10, \n",
    "         figsize     = (15, 6))\n",
    "\n",
    "# plt.xscale('log')\n",
    "\n",
    "# plt.ylim([0.1, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis\n",
    "\n",
    "- Derrac, J., García, S., Molina, D., & Herrera, F. (2011). A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms. Swarm and Evolutionary Computation, 1(1), 3-18.\n",
    "\n",
    "- García, S., Fernández, A., Luengo, J., & Herrera, F. (2010). Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power. Information sciences, 180(10), 2044-2064."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Friedman Aligned Ranking (FAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, p_value, rankings_avg, rankings_cmp = friedman_aligned_ranks_test( df )\n",
    "\n",
    "\n",
    "# Summary\n",
    "#\n",
    "print('\\n')\n",
    "print('[INFO] H0: {All methods exhibited similar results with no statistical differences}')\n",
    "print('[INFO] FAR: %.3f (p-value: %.5f)' % (T, p_value))\n",
    "if (p_value < 0.05):\n",
    "    print('\\t> H0 is rejected')\n",
    "else:\n",
    "    print('\\t> H0 is failed to be rejected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAR ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ranking            = pd.DataFrame( [] )\n",
    "Ranking['Methods'] = df.columns\n",
    "Ranking['FAR']     = rankings_avg\n",
    "\n",
    "Ranking = Ranking.sort_values(by           = 'FAR', \n",
    "                              ignore_index = True)\n",
    "Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finner post-hoc test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with rankings\n",
    "#\n",
    "d = {}\n",
    "for i, feature in enumerate( df.columns ):\n",
    "    d[ feature ] = rankings_cmp[i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple comparisons ($1 \\times N$)\n",
    "\n",
    "**Finner post-hoc test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons, z_values, p_values, adj_p_values = finner_test( d )\n",
    "\n",
    "Finner = pd.DataFrame( [] )\n",
    "Finner['Comparisons']     = comparisons\n",
    "Finner['APV']             = adj_p_values\n",
    "Finner['Null hypothesis'] = Finner['APV'].apply(lambda x: 'Rejected' if x < 0.05 else 'Failed to reject')\n",
    "\n",
    "Finner = Finner.sort_values(by = 'APV', ascending = False, ignore_index = True)\n",
    "Finner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking['Methods'] = ['Seq2Seq LSTM (Hybrid)', 'Seq2Seq LSTM + (d-diff)', 'NBeats', 'NBeats + (d-diff)', 'RF + (d-diff)', 'Seq2Seq LSTM', 'RF']\n",
    "# #\n",
    "# Ranking = Ranking.sort_values(by = 'FAR', ignore_index = True)\n",
    "# Ranking['APV'] = '-'\n",
    "# Ranking['APV'][1:] = Finner['APV']\n",
    "# Ranking['Null hypothesis'] = '-'\n",
    "# Ranking['Null hypothesis'][1:] = Finner['Null hypothesis']\n",
    "# Ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple comparisons ($N \\times N$)\n",
    "\n",
    "**Finner-multitest post-hoc test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons, z_values, p_values, adj_p_values = finner_multitest(d)\n",
    "\n",
    "# Finner = pd.DataFrame( [] )\n",
    "# Finner['Comparisons']     = comparisons\n",
    "# Finner['APV']             = adj_p_values\n",
    "# Finner['Null hypothesis'] = Finner['APV'].apply(lambda x: 'Rejected' if x < 0.05 else 'Failed to reject')\n",
    "\n",
    "# Finner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286.66070556640625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
